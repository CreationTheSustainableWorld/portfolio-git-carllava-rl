import os
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from PIL import Image
from transformers import set_seed
from git_rl_carllava_model import GitRLCarllavaModel

# === ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
set_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EPISODES = 10
GAMMA = 0.99
LR = 1e-4
EPS_CLIP = 0.2
K_EPOCHS = 3
POLICY_SAVE_PATH = "policy_head_rl_latest.pth"

# === ç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– ===
env = gym.make("CarRacing-v2", render_mode="human", continuous=False)
model = GitRLCarllavaModel(model_path="lora_git_caption_model_carllava").to(device)

# === ä»¥å‰ã®policy_headãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã‚€ ===
if os.path.exists(POLICY_SAVE_PATH):
    model.policy_head.load_state_dict(torch.load(POLICY_SAVE_PATH))
    print(f"ğŸ“¥ å‰å›ã® policy_head ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {POLICY_SAVE_PATH}")
else:
    print("ğŸ†• æ–°ã—ã„ policy_head ã‹ã‚‰å­¦ç¿’é–‹å§‹")

optimizer = optim.Adam(model.policy_head.parameters(), lr=LR)

# === ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’ãƒ«ãƒ¼ãƒ— ===
for episode in range(EPISODES):
    obs, _ = env.reset()
    done = False
    rewards = []
    log_probs = []
    actions = []
    states = []

    while not done:
        image = Image.fromarray(obs).convert("RGB")

        logits = model(image)
        dist = torch.distributions.Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        obs, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        rewards.append(reward)
        log_probs.append(log_prob.detach())
        actions.append(action.detach())
        states.append(image)

    # === å‰²å¼•å ±é…¬ã®è¨ˆç®— ===
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + GAMMA * G
        returns.insert(0, G)
    returns = torch.tensor(returns).float().to(device)
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)

    # === PPOæ›´æ–° ===
    model.train()
    for _ in range(K_EPOCHS):
        for i in range(len(states)):
            logits = model(states[i])
            dist = torch.distributions.Categorical(logits=logits)
            new_log_prob = dist.log_prob(actions[i])

            ratio = torch.exp(new_log_prob - log_probs[i])
            advantage = returns[i]

            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * advantage
            loss = -torch.min(surr1, surr2).mean()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    print(f"âœ… Episode {episode+1}, Total Reward: {sum(rewards):.2f}")

    # === policy_head ã®ä¿å­˜ ===
    torch.save(model.policy_head.state_dict(), POLICY_SAVE_PATH)
    print(f"ğŸ’¾ policy_head ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {POLICY_SAVE_PATH}")

env.close()
