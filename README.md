# ğŸ§  CarLLaVA-RL: Vision-Language Guided Reinforcement Learning with LoRA

This project demonstrates how to use a LoRA-fine-tuned vision-language model (Git) in the style of CarLLaVA to guide reinforcement learning (PPO) in the [CarRacing-v2](https://www.gymlibrary.dev/environments/box2d/car_racing/) environment.

---

## ğŸš— Overview

We leverage the Git vision-language model and fine-tune it using a small dataset of driving scenarios paired with natural language action descriptions like:

> What should the vehicle do in this scenario? â†’ "Turn left" / "Accelerate" / "Brake"

Once trained, the model is used as a policy in a PPO loop to learn autonomous driving behavior in a simulated environment.

---

## ğŸ§± Project Structure

carllava-rl/ â”œâ”€â”€ train_git_lora_carllava_style.py # LoRA fine-tuning with fixed prompts â”œâ”€â”€ ppo_git_with_prompt.py # Reinforcement Learning with PPO â”œâ”€â”€ GitText2ActionWithPrompt.py # Model class: Git + Prompt + Policy head â”œâ”€â”€ CarRacing-Data/ # Collected caption dataset â”œâ”€â”€ lora_git_caption_model_carllava/ # Fine-tuned Git model â”œâ”€â”€ results/ # Training graphs and videos â””â”€â”€ README.md

yaml
ã‚³ãƒ”ãƒ¼ã™ã‚‹
ç·¨é›†ã™ã‚‹

---

## ğŸ“Š Performance

| Epoch | Avg Loss (LoRA) | Total Reward (PPO) |
|-------|------------------|--------------------|
| 1     | 8.59             | -28.3              |
| 2     | 8.11             | -11.8              |
| 3     | 8.06             | +22.6 ğŸš€           |

### Reward Curve

![Reward Curve](results/reward_curve.png)

---

## ğŸ® Demo (Video)

> Below is the video of the trained agent using the Git model with natural language prompts:

https://user-images.githubusercontent.com/your-username/video_episode_10.mp4

---

## ğŸ”§ Key Features

- ğŸ” **Caption Collection**: From trained policy (image â†’ natural language)
- ğŸ§  **LoRA Fine-tuning**: Git model fine-tuned on small caption dataset
- ğŸ¤– **Prompt-guided RL**: "What should the vehicle do in this scenario?" as fixed input prompt
- ğŸï¸ **Control Output**: Logits for discrete driving actions (no-op, accelerate, left, right, brake)

---

## ğŸš€ Performance Improvements

The following changes improved the training performance significantly:

- âœ… Return normalization
- âœ… Dropout & hidden layer expansion in policy head
- âœ… Clipped rewards for stability
- âœ… Multiple PPO epochs (`K_EPOCHS = 5`)

---

## ğŸ’¾ Setup

You can install dependencies using conda:

```bash
conda env create -f environment.yml
conda activate carllava-rl
Or use requirements.txt.

ğŸ“š References
CarLLaVA (arXiv)

Git Model (Hugging Face)

LoRA (PEFT)
